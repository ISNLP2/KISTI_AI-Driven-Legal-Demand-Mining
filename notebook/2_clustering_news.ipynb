{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[뉴스·SNS 기반 입법 수요 탐지 - SBERT + 고도화 키워드 + 정량지표 통합 버전]\n",
    "\n",
    "- 임베딩: SBERT\n",
    "- 차원축소/클러스터링: UMAP + HDBSCAN\n",
    "- 키워드: c-TF-IDF → 조사/어미 제거 + 중복 제거 → SBERT 기반 재랭킹(MMR)\n",
    "- 대표문장/대표타이틀: SBERT 임베딩 중심성 기반\n",
    "- 정량지표:\n",
    "  · 전역(카테고리): DBCV, Silhouette, Noise ratio, n_clusters, mean_membership_prob, persistence_mean/median\n",
    "  · 군집: intra_sim_mean, inter_sim_max, separation_margin, keyword_npmi, keyword_diversity\n",
    "  · 이슈 중요도(salience_score): 빈도·성장률·참여·키워드결속도·분리마진 가중합\n",
    "\n",
    "- 결과: CSV / JSON / Excel\n",
    "  · {base}_table.csv            : 군집별 상세 + 정량지표 + 중요도\n",
    "  · {base}_outputs.json         : 예시 출력(JSON)\n",
    "  · {base}_cat_metrics.csv      : 카테고리 전역 지표\n",
    "\n",
    "-------------------------------------------------\n",
    "[Conda 환경 설치 가이드]\n",
    "```\n",
    "conda create -n law-issue python=3.10 -y\n",
    "conda activate law-issue\n",
    "conda install -c conda-forge numpy pandas scikit-learn openpyxl umap-learn hdbscan -y\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install sentence-transformers tqdm\n",
    "```\n",
    "-------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NM5cJpAeNAQX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing category group: 중대재해처벌법 | size=6969\n",
      "[INFO] Loading SBERT model: paraphrase-multilingual-mpnet-base-v2 on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jsk45\\anaconda3\\envs\\law-issue\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "c:\\Users\\jsk45\\anaconda3\\envs\\law-issue\\lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jsk45\\anaconda3\\envs\\law-issue\\lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Excel file saved: preprocess_news_part_4_with_metrics_table.xlsx\n",
      "[OK] Saved table/json/cat-metrics for preprocess_news_part_4_with_metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json, argparse, warnings\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, umap, hdbscan\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# SBERT 임베딩\n",
    "# -----------------------------\n",
    "_SBERT_MODEL = None\n",
    "def embed_texts_sbert(texts, model_name=\"paraphrase-multilingual-mpnet-base-v2\",\n",
    "                      batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    SBERT 임베딩 (정규화 포함)\n",
    "    \"\"\"\n",
    "    global _SBERT_MODEL\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if _SBERT_MODEL is None:\n",
    "        print(f\"[INFO] Loading SBERT model: {model_name} on {device}\")\n",
    "        _SBERT_MODEL = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    emb = _SBERT_MODEL.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    return emb\n",
    "\n",
    "# -----------------------------\n",
    "# 조사/어미 제거 + 토크나이즈\n",
    "# -----------------------------\n",
    "_JOSA_SUFFIXES = [\n",
    "    \"은\",\"는\",\"이\",\"가\",\"을\",\"를\",\"과\",\"와\",\"도\",\"만\",\"으로\",\"로\",\"에서\",\"에게\",\"한테\",\n",
    "    \"께\",\"에\",\"의\",\"라고\",\"라는\",\"이라\",\"이나\",\"이며\",\"인데\",\"하다\",\"했다\",\"하는\",\"되다\",\"된다\",\"되는\"\n",
    "]\n",
    "\n",
    "def strip_josa(token):\n",
    "    for suf in sorted(_JOSA_SUFFIXES, key=len, reverse=True):\n",
    "        if token.endswith(suf) and len(token) > len(suf)+1:\n",
    "            return token[: -len(suf)]\n",
    "    return token\n",
    "\n",
    "def tokenize_ko(text):\n",
    "    raw = re.findall(r\"[가-힣A-Za-z0-9]{2,}\", text)\n",
    "    toks = []\n",
    "    for t in raw:\n",
    "        base = strip_josa(t)\n",
    "        if base and len(base) >= 2:\n",
    "            toks.append(base)\n",
    "    return toks\n",
    "\n",
    "def reduce_terms_by_containment(terms, top_k=8):\n",
    "    out = []\n",
    "    for t in terms:\n",
    "        if any((t == s) or (t in s) or (s in t) for s in out):\n",
    "            continue\n",
    "        out.append(t)\n",
    "        if len(out) >= top_k:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# SBERT 기반 키워드 재랭킹 (MMR)\n",
    "# -----------------------------\n",
    "def rerank_keywords_with_sbert(cluster_texts, candidates, model_name=\"paraphrase-multilingual-mpnet-base-v2\",\n",
    "                               top_k=8, diversity=0.7):\n",
    "    if not candidates:\n",
    "        return []\n",
    "    docs_emb = embed_texts_sbert(cluster_texts, model_name=model_name)\n",
    "    centroid = docs_emb.mean(axis=0, keepdims=True)\n",
    "    cands_emb = embed_texts_sbert(candidates, model_name=model_name)\n",
    "    sims = cosine_similarity(cands_emb, centroid).ravel()\n",
    "\n",
    "    selected, cand_idx = [], list(range(len(candidates)))\n",
    "    while cand_idx and len(selected) < top_k:\n",
    "        if not selected:\n",
    "            i = int(np.argmax(sims[cand_idx]))\n",
    "            selected.append(cand_idx.pop(i))\n",
    "        else:\n",
    "            sel_emb = cands_emb[selected]\n",
    "            div = cosine_similarity(cands_emb[cand_idx], sel_emb).max(axis=1)\n",
    "            mmr = (1 - diversity) * sims[cand_idx] - diversity * div\n",
    "            i = int(np.argmax(mmr))\n",
    "            selected.append(cand_idx.pop(i))\n",
    "    return [candidates[i] for i in selected]\n",
    "\n",
    "# -----------------------------\n",
    "# 대표문장 (SBERT 기반)\n",
    "# -----------------------------\n",
    "def pick_representative_sentence_by_sbert(texts, model_name=\"paraphrase-multilingual-mpnet-base-v2\"):\n",
    "    if not texts:\n",
    "        return \"\"\n",
    "    emb = embed_texts_sbert(texts, model_name=model_name)\n",
    "    centroid = emb.mean(axis=0, keepdims=True)\n",
    "    d = cosine_distances(emb, centroid).ravel()\n",
    "    return texts[int(np.argmin(d))]\n",
    "\n",
    "# -----------------------------\n",
    "# c-TF-IDF (후보 키워드 추출)\n",
    "# -----------------------------\n",
    "def compute_c_tf_idf(docs_per_cluster, ngram_range=(1, 3), min_df=2, epsilon=1e-9):\n",
    "    cluster_ids = sorted(docs_per_cluster.keys())\n",
    "    cluster_docs = [\" \".join(docs_per_cluster[cid]) for cid in cluster_ids]\n",
    "    preproc_docs = [\" \".join(tokenize_ko(doc)) for doc in cluster_docs]\n",
    "\n",
    "    cv = CountVectorizer(ngram_range=ngram_range, min_df=min_df)\n",
    "    X = cv.fit_transform(preproc_docs)\n",
    "    terms = np.array(cv.get_feature_names_out())\n",
    "\n",
    "    df_t = (X > 0).sum(axis=0).A1\n",
    "    idf = np.log((len(cluster_ids) + 1) / (df_t + epsilon))\n",
    "    row_sums = np.asarray(X.sum(axis=1)).ravel()\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    X_norm = X.multiply(1.0 / row_sums[:, None])\n",
    "    ctfidf_mat = X_norm.multiply(idf)\n",
    "\n",
    "    ctfidf = {}\n",
    "    for i, cid in enumerate(cluster_ids):\n",
    "        row = ctfidf_mat.getrow(i).toarray().ravel()\n",
    "        order = np.argsort(row)[::-1]\n",
    "        ctfidf[cid] = (terms[order], row[order])\n",
    "    return ctfidf\n",
    "\n",
    "# -----------------------------\n",
    "# 시간/텍스트 유틸\n",
    "# -----------------------------\n",
    "def to_datetime_ymdhms(x):\n",
    "    try:\n",
    "        return pd.to_datetime(str(int(x)), format=\"%Y%m%d%H%M%S\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.to_datetime(x)\n",
    "        except Exception:\n",
    "            return pd.NaT\n",
    "\n",
    "def build_text(row, max_len=600):\n",
    "    t = row.get(\"title_norm\") if pd.notna(row.get(\"title_norm\")) else row.get(\"title\", \"\")\n",
    "    c = row.get(\"content_norm\") if pd.notna(row.get(\"content_norm\")) else row.get(\"content\", \"\")\n",
    "    return (str(t) + \" \" + str(c)[:max_len]).strip()\n",
    "\n",
    "def safe_sum_frame(sub_df, cols):\n",
    "    if not cols: return 0\n",
    "    vals = sub_df[cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).sum(axis=1)\n",
    "    return int(vals.sum())\n",
    "\n",
    "def find_cols(df, key): return [c for c in df.columns if key in c.lower()]\n",
    "\n",
    "def compute_growth(sub_df, date_col=\"date_dt\", window_days=7):\n",
    "    if date_col not in sub_df.columns or sub_df[date_col].isna().all():\n",
    "        return np.nan\n",
    "    sub = sub_df.dropna(subset=[date_col]).copy()\n",
    "    if sub.empty: return np.nan\n",
    "    t_max = sub[date_col].max()\n",
    "    recent_start, prev_start = t_max - timedelta(days=window_days), t_max - timedelta(days=2*window_days)\n",
    "    recent = sub[(sub[date_col] > recent_start) & (sub[date_col] <= t_max)]\n",
    "    prev = sub[(sub[date_col] > prev_start) & (sub[date_col] <= recent_start)]\n",
    "    return (len(recent) - max(1, len(prev))) / max(1, len(prev))\n",
    "\n",
    "# -----------------------------\n",
    "# 정량지표: 전역/군집 품질\n",
    "# -----------------------------\n",
    "def cluster_validity_indices(emb_umap, labels):\n",
    "    # Silhouette (노이즈 제외)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() > 1 and len(np.unique(labels[mask])) > 1:\n",
    "        sil = float(silhouette_score(emb_umap[mask], labels[mask], metric=\"euclidean\"))\n",
    "    else:\n",
    "        sil = np.nan\n",
    "    return {\"silhouette\": sil}\n",
    "\n",
    "def hdbscan_global_validity(emb_raw, labels):\n",
    "    # HDBSCAN 전용 DBCV (원 임베딩 권장)\n",
    "    try:\n",
    "        v = float(hdbscan.validity_index(emb_raw, labels))\n",
    "    except Exception:\n",
    "        v = np.nan\n",
    "    return {\"dbcv\": v}\n",
    "\n",
    "def cluster_membership_stats(probabilities, labels, clusterer):\n",
    "    # 확률/안정성(퍼시스턴스) 통계\n",
    "    mask = labels != -1\n",
    "    mean_prob = float(np.nanmean(probabilities[mask])) if mask.any() else np.nan\n",
    "    persist = getattr(clusterer, \"cluster_persistence_\", None)\n",
    "    if persist is not None and len(persist) > 0:\n",
    "        persist_mean = float(np.mean(persist))\n",
    "        persist_median = float(np.median(persist))\n",
    "    else:\n",
    "        persist_mean = persist_median = np.nan\n",
    "    noise_ratio = float(np.mean(labels == -1)) if len(labels) > 0 else np.nan\n",
    "    n_clusters = int(len(np.unique(labels[labels!=-1])))\n",
    "    return {\n",
    "        \"mean_membership_prob\": mean_prob,\n",
    "        \"persistence_mean\": persist_mean,\n",
    "        \"persistence_median\": persist_median,\n",
    "        \"noise_ratio\": noise_ratio,\n",
    "        \"n_clusters\": n_clusters,\n",
    "    }\n",
    "\n",
    "def per_cluster_intra_inter_similarity(emb_raw, labels):\n",
    "    \"\"\"\n",
    "    군집별:\n",
    "      - intra_sim_mean: 군집 내 평균 코사인 유사도\n",
    "      - inter_sim_max: 해당 군집과 타 군집 간 최대 코사인 유사도 (낮을수록 분리 양호)\n",
    "      - separation_margin = intra_sim_mean - inter_sim_max\n",
    "    \"\"\"\n",
    "    sims = cosine_similarity(emb_raw)\n",
    "    stats = {}\n",
    "    cluster_ids = [cid for cid in np.unique(labels) if cid != -1]\n",
    "    idx_per = {cid: np.where(labels==cid)[0] for cid in cluster_ids}\n",
    "    others = np.where(labels != -1)[0]\n",
    "\n",
    "    for cid in cluster_ids:\n",
    "        idx = idx_per[cid]\n",
    "        # intra\n",
    "        if len(idx) >= 2:\n",
    "            S = sims[np.ix_(idx, idx)]\n",
    "            intra_mean = float(np.mean(S[np.triu_indices_from(S, k=1)]))\n",
    "        elif len(idx) == 1:\n",
    "            intra_mean = np.nan\n",
    "        else:\n",
    "            intra_mean = np.nan\n",
    "\n",
    "        # inter\n",
    "        other_idx = np.where((labels!=-1) & (labels!=cid))[0]\n",
    "        if len(idx)>0 and len(other_idx)>0:\n",
    "            S2 = sims[np.ix_(idx, other_idx)]\n",
    "            inter_max = float(np.max(S2))\n",
    "        else:\n",
    "            inter_max = np.nan\n",
    "\n",
    "        if (not np.isnan(intra_mean)) and (not np.isnan(inter_max)):\n",
    "            sep_margin = float(intra_mean - inter_max)\n",
    "        else:\n",
    "            sep_margin = np.nan\n",
    "\n",
    "        stats[int(cid)] = {\n",
    "            \"intra_sim_mean\": intra_mean,\n",
    "            \"inter_sim_max\": inter_max,\n",
    "            \"separation_margin\": sep_margin\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "# -----------------------------\n",
    "# 키워드 결속도(NPMI) & 다양성\n",
    "# -----------------------------\n",
    "def keyword_npmi_coherence(corpus_tokens, top_terms, window=None):\n",
    "    \"\"\"\n",
    "    간단 NPMI: 문서 수준 공출현(윈도우 생략) 기반\n",
    "    \"\"\"\n",
    "    from math import log\n",
    "    if not top_terms or len(top_terms) < 2:\n",
    "        return np.nan\n",
    "    N = len(corpus_tokens)\n",
    "    if N == 0: return np.nan\n",
    "\n",
    "    terms = top_terms[:min(10, len(top_terms))]\n",
    "    pairs = [(a,b) for i,a in enumerate(terms) for b in terms[i+1:]]\n",
    "\n",
    "    def term_in_doc(t, toks): return t in toks\n",
    "    def pair_in_doc(a,b,toks): return (a in toks) and (b in toks)\n",
    "\n",
    "    pa = {t: (sum(term_in_doc(t, d) for d in corpus_tokens)+1) / (N+1) for t in terms}\n",
    "    cab = {}\n",
    "    for a,b in pairs:\n",
    "        cab[(a,b)] = (sum(pair_in_doc(a,b,d) for d in corpus_tokens)+1) / (N+1)\n",
    "\n",
    "    npmies = []\n",
    "    for a,b in pairs:\n",
    "        p_ab = cab[(a,b)]\n",
    "        p_a, p_b = pa[a], pa[b]\n",
    "        if p_ab <= 0 or p_ab >= 1.0:\n",
    "            continue  # log(1)=0 → 분모 0 에러 방지\n",
    "        pmi = log(p_ab / (p_a*p_b))\n",
    "        npmi = pmi / (-log(p_ab))\n",
    "        npmies.append(npmi)\n",
    "    return float(np.mean(npmies)) if npmies else np.nan\n",
    "\n",
    "def keyword_diversity_score(terms):\n",
    "    if not terms: return np.nan\n",
    "    uniq = set(terms)\n",
    "    diversity = len(uniq) / max(1, len(terms))\n",
    "    return float(diversity)\n",
    "\n",
    "# -----------------------------\n",
    "# 중요도(우선순위) 종합 점수\n",
    "# -----------------------------\n",
    "def zscore(x):\n",
    "    x = np.array(x, dtype=float)\n",
    "    m = np.nanmean(x); s = np.nanstd(x)\n",
    "    return (x - m) / (s + 1e-9)\n",
    "\n",
    "def compute_salience_per_cluster(rows):\n",
    "    \"\"\"\n",
    "    rows: 같은 카테고리 내 군집 dict들의 리스트\n",
    "    각 dict에 'salience_score'를 삽입\n",
    "    \"\"\"\n",
    "    F = np.array([r.get(\"cluster_size\", np.nan) for r in rows])\n",
    "    G = np.array([r.get(\"growth_rate(7d/prev7d)\", np.nan) for r in rows])\n",
    "    E = np.array([ (r.get(\"eng_like\",0)+r.get(\"eng_reply\",0)+r.get(\"eng_share\",0)) for r in rows], dtype=float)\n",
    "    C = np.array([r.get(\"keyword_npmi\", np.nan) for r in rows])\n",
    "    M = np.array([r.get(\"separation_margin\", np.nan) for r in rows])\n",
    "\n",
    "    score = 0.35*zscore(F) + 0.25*zscore(G) + 0.20*zscore(E) + 0.10*zscore(C) + 0.10*zscore(M)\n",
    "    for i, r in enumerate(rows):\n",
    "        r[\"salience_score\"] = float(score[i])\n",
    "    return rows\n",
    "\n",
    "# -----------------------------\n",
    "# 메인 파이프라인\n",
    "# -----------------------------\n",
    "def run_pipeline_by_category(\n",
    "    excel_path: str,\n",
    "    sbert_model=\"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    umap_n_components=10,\n",
    "    umap_n_neighbors=15,\n",
    "    umap_min_dist=0.1,\n",
    "    hdbscan_min_cluster_size=25,\n",
    "    hdbscan_min_samples=10,\n",
    "    ctfidf_top_k=8,\n",
    "    seed=42\n",
    "):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    if \"date\" in df.columns: df[\"date_dt\"] = df[\"date\"].apply(to_datetime_ymdhms)\n",
    "    else: df[\"date_dt\"] = pd.NaT\n",
    "    df[\"text\"] = df.apply(build_text, axis=1)\n",
    "    df = df[df[\"text\"].str.len() > 0].copy()\n",
    "\n",
    "    results, json_outputs, cat_metrics_rows = [], [], []\n",
    "\n",
    "    for cat, g in df.groupby(\"category\"):\n",
    "        if not isinstance(cat, str) or len(cat.strip()) == 0: \n",
    "            continue\n",
    "        print(f\"[INFO] Processing category group: {cat} | size={len(g)}\")\n",
    "        if len(g) < 10: \n",
    "            continue\n",
    "\n",
    "        # ---------------- Embedding & UMAP ----------------\n",
    "        emb = embed_texts_sbert(g[\"text\"].tolist(), model_name=sbert_model)\n",
    "        reducer = umap.UMAP(n_neighbors=umap_n_neighbors, n_components=umap_n_components,\n",
    "                            min_dist=umap_min_dist, metric=\"cosine\", random_state=seed)\n",
    "        X_umap = reducer.fit_transform(emb)\n",
    "        X_umap_df = pd.DataFrame(X_umap, index=g.index)\n",
    "\n",
    "        # ---------------- HDBSCAN ----------------\n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=hdbscan_min_cluster_size,\n",
    "            min_samples=hdbscan_min_samples,\n",
    "            metric=\"euclidean\",\n",
    "            prediction_data=True\n",
    "        )\n",
    "        labels = clusterer.fit_predict(X_umap_df.values)\n",
    "        g = g.copy(); g[\"cluster_id\"] = labels\n",
    "        prob = getattr(clusterer, \"probabilities_\", np.ones(len(g)))\n",
    "\n",
    "        # ---- 전역(카테고리) 지표 ----\n",
    "        glob = {}\n",
    "        glob.update(cluster_validity_indices(X_umap_df.values, labels))\n",
    "        glob.update(hdbscan_global_validity(emb, labels))       # 원 임베딩 기준 권장\n",
    "        glob.update(cluster_membership_stats(prob, labels, clusterer))\n",
    "\n",
    "        cat_metrics_rows.append({\n",
    "            \"category\": cat,\n",
    "            **glob\n",
    "        })\n",
    "\n",
    "        # ---- 군집별 분리/응집 (원 임베딩 기준) ----\n",
    "        per_c_stats = per_cluster_intra_inter_similarity(emb, labels)\n",
    "\n",
    "        # ---- c-TF-IDF 후보 키워드 ----\n",
    "        docs_per_cluster = {int(cid): sub[\"text\"].tolist() for cid, sub in g.groupby(\"cluster_id\") if cid != -1}\n",
    "        if not docs_per_cluster: \n",
    "            continue\n",
    "        ctfidf = compute_c_tf_idf(docs_per_cluster, ngram_range=(1,3), min_df=2)\n",
    "\n",
    "        # ---- 문서 토큰 캐시 (키워드 결속도용) ----\n",
    "        doc_tokens = [tokenize_ko(t) for t in g[\"text\"].tolist()]\n",
    "        idx_to_pos = {idx:i for i,idx in enumerate(g.index)}\n",
    "        doc_tokens_by_cluster = defaultdict(list)\n",
    "        for idx, cid in zip(g.index, labels):\n",
    "            if cid != -1:\n",
    "                pos = idx_to_pos[idx]\n",
    "                doc_tokens_by_cluster[int(cid)].append(doc_tokens[pos])\n",
    "\n",
    "        # ---- 군집 루프 ----\n",
    "        for cid, sub in g.groupby(\"cluster_id\"):\n",
    "            if cid == -1: \n",
    "                continue\n",
    "            size = len(sub)\n",
    "\n",
    "            # 키워드 (ctfidf → 중복/포함 제거 → SBERT-MMR 재랭킹)\n",
    "            terms, _ = ctfidf.get(int(cid), (np.array([]), np.array([])))\n",
    "            raw_terms = reduce_terms_by_containment(terms.tolist(), top_k=50)\n",
    "            keywords = rerank_keywords_with_sbert(sub[\"text\"].tolist(), raw_terms, model_name=sbert_model,\n",
    "                                                  top_k=ctfidf_top_k, diversity=0.7)\n",
    "\n",
    "            # 대표 문장 & 대표 타이틀\n",
    "            rep_sent = pick_representative_sentence_by_sbert(sub[\"text\"].tolist(), model_name=sbert_model)\n",
    "            sub_emb = embed_texts_sbert(sub[\"text\"].tolist(), model_name=sbert_model)\n",
    "            centroid = sub_emb.mean(axis=0, keepdims=True)\n",
    "            d = cosine_distances(sub_emb, centroid).ravel()\n",
    "            rep_titles = sub.iloc[np.argsort(d)[:3]][\"title\"].tolist()\n",
    "\n",
    "            # 성장/참여도\n",
    "            growth_rate = compute_growth(sub)\n",
    "            like_total  = safe_sum_frame(sub, find_cols(sub, \"like\"))\n",
    "            reply_total = safe_sum_frame(sub, [c for c in sub.columns if (\"reply\" in c.lower() or \"comment\" in c.lower())])\n",
    "            share_total = safe_sum_frame(sub, find_cols(sub, \"share\"))\n",
    "\n",
    "            # 군집 품질 지표\n",
    "            stats_c = per_c_stats.get(int(cid), {\"intra_sim_mean\":np.nan,\"inter_sim_max\":np.nan,\"separation_margin\":np.nan})\n",
    "\n",
    "            # 키워드 결속/다양성\n",
    "            corpus_c = doc_tokens_by_cluster.get(int(cid), [])\n",
    "            kw_npmi = keyword_npmi_coherence(corpus_c, keywords)\n",
    "            kw_div  = keyword_diversity_score(keywords)\n",
    "\n",
    "            # JSON 예시 출력\n",
    "            json_outputs.append({\n",
    "                \"example_output\": {\n",
    "                    \"category\": cat, \"cluster_id\": int(cid),\n",
    "                    \"issue_summary\": {\"keywords\": keywords, \"representative_sentence\": rep_sent},\n",
    "                    \"representative_titles\": rep_titles, \"size\": size\n",
    "                },\n",
    "                \"metrics\": {\n",
    "                    \"freq\": size,\n",
    "                    \"growth_rate\": None if pd.isna(growth_rate) else float(np.round(growth_rate,4)),\n",
    "                    \"engagement\": {\"share\": int(share_total), \"reply\": int(reply_total), \"like\": int(like_total)}\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # CSV 결과 행\n",
    "            results.append({\n",
    "                \"category\": cat, \"cluster_id\": int(cid),\n",
    "                \"keywords\": \" \".join(keywords),\n",
    "                \"representative_sentence\": rep_sent,\n",
    "                \"rep_title_1\": rep_titles[0] if len(rep_titles)>0 else \"\",\n",
    "                \"rep_title_2\": rep_titles[1] if len(rep_titles)>1 else \"\",\n",
    "                \"rep_title_3\": rep_titles[2] if len(rep_titles)>2 else \"\",\n",
    "                \"cluster_size\": size,\n",
    "                \"growth_rate(7d/prev7d)\": None if pd.isna(growth_rate) else float(np.round(growth_rate,4)),\n",
    "                \"eng_like\": int(like_total),\"eng_reply\": int(reply_total),\"eng_share\": int(share_total),\n",
    "\n",
    "                # 전역 지표(카테고리 수준) 부착\n",
    "                \"dbcv\": glob[\"dbcv\"],\n",
    "                \"silhouette\": glob[\"silhouette\"],\n",
    "                \"mean_membership_prob\": glob[\"mean_membership_prob\"],\n",
    "                \"persistence_mean\": glob[\"persistence_mean\"],\n",
    "                \"persistence_median\": glob[\"persistence_median\"],\n",
    "                \"noise_ratio\": glob[\"noise_ratio\"],\n",
    "                \"n_clusters\": glob[\"n_clusters\"],\n",
    "\n",
    "                # 군집 지표\n",
    "                \"intra_sim_mean\": stats_c[\"intra_sim_mean\"],\n",
    "                \"inter_sim_max\": stats_c[\"inter_sim_max\"],\n",
    "                \"separation_margin\": stats_c[\"separation_margin\"],\n",
    "                \"keyword_npmi\": kw_npmi,\n",
    "                \"keyword_diversity\": kw_div,\n",
    "            })\n",
    "\n",
    "        # ---- 같은 카테고리 내 군집들의 salience 산출 ----\n",
    "        # (결과 리스트 중 해당 카테고리 행들만 추출하여 가중합 스코어 부여)\n",
    "        idxs = [i for i,r in enumerate(results) if r[\"category\"]==cat]\n",
    "        rows = [results[i] for i in idxs]\n",
    "        rows = compute_salience_per_cluster(rows)\n",
    "        for off, i in enumerate(idxs):\n",
    "            results[i] = rows[off]\n",
    "\n",
    "    # ---------------- 저장 ----------------\n",
    "    base = os.path.splitext(os.path.basename(excel_path))[0]\n",
    "    # 1) 군집 테이블 (Excel 저장)\n",
    "    df_results = pd.DataFrame(results).sort_values(\n",
    "        [\"category\",\"salience_score\",\"cluster_size\"], ascending=[True, False, False]\n",
    "    )\n",
    "    df_results = df_results.rename(columns={\"salience_score\": \"final_rank_score\"})\n",
    "    excel_path_out = f\"{base}_table.xlsx\"\n",
    "    df_results.to_excel(excel_path_out, index=False, engine=\"openpyxl\")\n",
    "    print(f\"[OK] Excel file saved: {excel_path_out}\")\n",
    "    # 2) JSON 예시\n",
    "    with open(f\"{base}_outputs.json\", \"w\", encoding=\"utf-8\") as f: \n",
    "        json.dump(json_outputs, f, ensure_ascii=False, indent=2)\n",
    "    # 3) 카테고리 전역 지표\n",
    "    pd.DataFrame(cat_metrics_rows).sort_values([\"category\"]).to_csv(\n",
    "        f\"{base}_cat_metrics.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    print(f\"[OK] Saved table/json/cat-metrics for {excel_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ---------------------------\n",
    "    # 직접 인자 지정 (argparse 제거)\n",
    "    # ---------------------------\n",
    "    EXCEL_PATH = \"preprocess_news_part_4_with_metrics.xlsx\" ## 입력 엑셀 파일 경로\n",
    "    SBERT_MODEL = \"paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "    run_pipeline_by_category(\n",
    "        excel_path=EXCEL_PATH,\n",
    "        sbert_model=SBERT_MODEL,\n",
    "        umap_n_components=10,\n",
    "        umap_n_neighbors=15,\n",
    "        umap_min_dist=0.1,\n",
    "        hdbscan_min_cluster_size=25,\n",
    "        hdbscan_min_samples=10,\n",
    "        ctfidf_top_k=8,\n",
    "        seed=42\n",
    "    )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "law-issue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
