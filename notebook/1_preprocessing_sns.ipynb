{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NM5cJpAeNAQX"
   },
   "outputs": [],
   "source": [
    "import re, unicodedata, html\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "EMOJI_SYMBOL_RE = re.compile(\n",
    "    \"[\"                         # BMP(비트맵 이미지 포맷)\n",
    "    \"\\u2190-\\u21FF\"             # arrows\n",
    "    \"\\u2300-\\u23FF\"             # misc technical\n",
    "    \"\\u2460-\\u24FF\"             # enclosed alphanumerics\n",
    "    \"\\u25A0-\\u25FF\"             # geometric shapes (■◆▲○●…)\n",
    "    \"\\u2600-\\u26FF\"             # misc symbols (☀☂☏…)\n",
    "    \"\\u2700-\\u27BF\"             # dingbats (✔✖✈…)\n",
    "    \"\\u2B00-\\u2BFF\"             # misc symbols & arrows\n",
    "    \"\\u3000-\\u303F\"             # CJK symbols (、。・《》【】…)\n",
    "    \"\\uFE0F\"                    # variation selector-16\n",
    "    \"\\u200d\"                    # zero width joiner\n",
    "    \"]\"\n",
    "    \"|\"\n",
    "    \"[\"                         # 보충 평면(astral)\n",
    "    \"\\U0001F000-\\U0001FAFF\"     # emoticons/pictographs/transport/etc.\n",
    "    \"\\U0001FB00-\\U0001FBFF\"\n",
    "    \"]\", flags=re.UNICODE)\n",
    "\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "HASHTAG_RE   = re.compile(r\"(?:^|(?<=\\s))#([^\\s#]+)\")     # '#청원' -> '청원'\n",
    "MENTION_RE= re.compile(r\"(?:^|(?<=\\s))@([A-Za-z0-9_.]+)\")\n",
    "\n",
    "# 제로폭/제어문자\n",
    "ZERO_WIDTH_RE = re.compile(r\"[\\u200B-\\u200D\\u2060\\uFEFF]\")\n",
    "\n",
    "# 뉴스 서명/홍보문구\n",
    "NEWS_TAILS_RE = re.compile(\n",
    "    r\"(자세한 내용은 (영상|뉴스|기사)로|구독과\\s*좋아요|무단 전재.*금지|\"\n",
    "    r\"YTN.*입니다\\.|KBS.*입니다\\.|MBC.*입니다\\.|연합뉴스TV.*입니다\\.|JTBC.*입니다\\.)\"\n",
    ")\n",
    "\n",
    "# 제보/문의/연락/카톡\n",
    "CONTACT_LINE_RE = re.compile(r\"(제보|문의|연락|카카오톡|KakaoTalk|전화|☎|Tel)[^\\n]*\", re.IGNORECASE)\n",
    "\n",
    "# 기자/앵커 서명\n",
    "REPORTER_SIGN_RE = re.compile(r\"\\b[\\uAC00-\\uD7A3]{2,5}\\s*기자(입니다\\.?)?\")\n",
    "\n",
    "# 반복문자/감탄/웃음\n",
    "REPEAT_PUNCT_RE = re.compile(r\"([!?.,])\\1{1,}\")   # !!??.. → ! ? .\n",
    "LAUGH_RE        = re.compile(r\"(ㅋ|ㅎ|ㅜ|ㅠ){3,}\") # ㅋㅋㅋㅋ → ㅋㅋ\n",
    "\n",
    "# 괄호 내용 삭제 기준\n",
    "NOISE_TAGS = [\"앵커\",\"기자\",\"리포트\",\"자막\",\"영상\",\"사진\",\"자료화면\",\"CG\",\"그래픽\",\n",
    "              \"브릿지\",\"현장연결\",\"속보\",\"뉴스\",\"단독\",\"취재\",\"연결\",\"보도국\",\n",
    "              \"제작지원\",\"협찬\",\"캡처\",\"출처\"]\n",
    "NOISE_TAGS_RE = re.compile(\"|\".join([re.escape(t) for t in NOISE_TAGS]))\n",
    "\n",
    "def keep_bracket(inner:str)->bool:\n",
    "    s = inner.strip()\n",
    "    if not s: return False\n",
    "    if re.search(r\"(법|조문|조항|제\\d+조|처벌|벌금|개정|입법|법안|고시|시행령|시행규칙)\", s): return True\n",
    "    if re.search(r\"(대법원|헌법재판소|행정심판|판결|판례|행안부|과기정통부|개인정보보호위원회|방통위)\", s): return True\n",
    "    if re.search(r\"\\b(19|20)\\d{2}\\b\", s): return True  # 연도\n",
    "    if NOISE_TAGS_RE.search(s): return False\n",
    "    return len(s) >= 3\n",
    "\n",
    "BRACKETS = [r\"\\[([^\\[\\]]+)\\]\", r\"\\(([^\\(\\)]+)\\)\", r\"\\<([^<>]+)\\)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NqRVgdOPRX_B"
   },
   "outputs": [],
   "source": [
    "# 법령명 별칭 정규화\n",
    "LAW_ALIAS = {\n",
    "    r\"\\b개보법\\b\": \"개인정보보호법\",\n",
    "    r\"\\bPIPA\\b\": \"개인정보보호법\",\n",
    "    r\"\\b망법\\b\": \"정보통신망 이용촉진 및 정보보호 등에 관한 법률\",\n",
    "    r\"정보통신망법\": \"정보통신망 이용촉진 및 정보보호 등에 관한 법률\",\n",
    "}\n",
    "LAW_ALIAS_COMPILED = [(re.compile(k), v) for k,v in LAW_ALIAS.items()]\n",
    "\n",
    "# 수요 의도 분류\n",
    "INTENT_PATTERNS = {\n",
    "    \"제정/신설\":  r\"(법(을)?\\s*만들|제정(하|해|하자)|입법\\s*하|신설)\",\n",
    "    \"개정/강화\":  r\"(개정(하|해|해야|안)|처벌\\s*강화|형량\\s*상향|벌금\\s*상향|처벌(을)?\\s*강하게)\",\n",
    "    \"완화/유연\":  r\"(규제\\s*완화|처벌\\s*완화|유연(하|해)|완화(하|해))\",\n",
    "    \"폐지/반대\":  r\"(폐지(하|해|하자)|법안\\s*반대|철회(하|해))\",\n",
    "    \"집행/감시\":  r\"(단속\\s*강화|집행\\s*강화|감시\\s*강화|시행령|시행규칙)\"\n",
    "}\n",
    "INTENT_RES = {k: re.compile(v) for k,v in INTENT_PATTERNS.items()}\n",
    "\n",
    "def normalize_law_aliases(s: str) -> str:\n",
    "    out = s\n",
    "    for rgx, repl in LAW_ALIAS_COMPILED:\n",
    "        out = rgx.sub(repl, out)\n",
    "    return out\n",
    "\n",
    "def extract_intents(s: str):\n",
    "    labels = [k for k, rx in INTENT_RES.items() if rx.search(s)]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zGf28MwLUwkN"
   },
   "outputs": [],
   "source": [
    "PUNCT_TRANSLATE = str.maketrans({\n",
    "    \"…\": \"...\", \"·\": \" \", \"•\": \" \", \"―\": \"-\", \"–\": \"-\", \"—\": \"-\",\n",
    "    \"“\": \"\\\"\", \"”\": \"\\\"\", \"‘\": \"'\", \"’\": \"'\",\n",
    "    \"◆\":\" \", \"■\":\" \", \"▲\":\" \", \"△\":\" \", \"▶\":\" \", \"▷\":\" \", \"▼\":\" \", \"▽\":\" \", \"※\":\" \"\n",
    "})\n",
    "\n",
    "def normalize_korean(text:str, keep_hashtag_tokens=True) -> str:\n",
    "    if not isinstance(text, str): return ''\n",
    "    t = html.unescape(text) # html entity, tag 흔적 정리\n",
    "    t = unicodedata.normalize(\"NFKC\", t) # 유니코드 정규화\n",
    "    t = unicodedata.normalize('NFC', t) # 한국어 일관된 형태(초성/중성 분해)\n",
    "    # 제로폭 제어\n",
    "    t = ZERO_WIDTH_RE.sub(\"\", t)\n",
    "\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = EMAIL_RE.sub(\" \", t)\n",
    "\n",
    "    # 해시태그/멘션: 토큰만 남길지 여부\n",
    "    if keep_hashtag_tokens:\n",
    "        t = HASHTAG_RE.sub(lambda m: \" \" + m.group(1), t)\n",
    "    else:\n",
    "        t = HASHTAG_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "\n",
    "    t = EMOJI_SYMBOL_RE.sub(\" \", t)\n",
    "    t = NEWS_TAILS_RE.sub(\" \", t)\n",
    "    t = CONTACT_LINE_RE.sub(\" \", t)\n",
    "    t = REPORTER_SIGN_RE.sub(\" \", t)\n",
    "\n",
    "    for patt in BRACKETS:\n",
    "        t = re.sub(patt, lambda m: (\" \" + m.group(1) + \" \") if keep_bracket(m.group(1)) else \" \", t)\n",
    "\n",
    "    t = REPEAT_PUNCT_RE.sub(r\"\\1\", t)\n",
    "    t = LAUGH_RE.sub(lambda m: m.group(1)*2, t)  # ㅋㅋㅋㅋ → ㅋㅋ\n",
    "\n",
    "    t = t.translate(PUNCT_TRANSLATE)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oPnMMltTWIUF"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def clean_domain(u: str) -> str:\n",
    "    if not isinstance(u, str) or not u.strip():\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = urlparse(u if u.startswith((\"http://\",\"https://\")) else \"http://\" + u)\n",
    "        return p.netloc.lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# 제목과 본문을 이용한 특성 key 부여(중복 데이터 확인용)\n",
    "def make_clean_hash(title_norm: str, content_norm: str) -> str:\n",
    "    key = (title_norm or \"\") + \"\\n\" + (content_norm or \"\")\n",
    "    return hashlib.md5(key.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x1Jd64hHN5QG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in ['제목','본문','url']:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].fillna(\"\").astype(str)\n",
    "        else:\n",
    "            out[c] = \"\"\n",
    "\n",
    "    out['title_norm']   = out['제목'].map(normalize_korean).map(normalize_law_aliases)\n",
    "    out['content_norm'] = out['본문'].map(normalize_korean).map(normalize_law_aliases)\n",
    "    out['clean_hash'] = [make_clean_hash(a,b) for a,b in zip(out['title_norm'], out['content_norm'])]\n",
    "    out['domain']     = out['url'].map(clean_domain)\n",
    "\n",
    "    out['intent_labels'] = (out['title_norm'] + \" \" + out['content_norm']).map(extract_intents)\n",
    "\n",
    "    # 한국어 비율, 본문 글자 수 측정\n",
    "    def ko_ratio(s: str) -> float:\n",
    "        if not s: return 0.0\n",
    "        ko = sum(1 for ch in s if '\\uAC00' <= ch <= '\\uD7A3')\n",
    "        return ko / max(1, len(s))\n",
    "    out['ko_ratio'] = out['content_norm'].map(ko_ratio)\n",
    "    out['len_chars'] = out['content_norm'].str.len()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_9867B25OAFM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== part 1 시작 ===\n",
      "=== part 1 완료 ===\n",
      "\n",
      "=== part 2 시작 ===\n",
      "=== part 2 완료 ===\n",
      "\n",
      "=== part 3 시작 ===\n",
      "=== part 3 완료 ===\n",
      "\n",
      "=== part 4 시작 ===\n",
      "=== part 4 완료 ===\n",
      "\n",
      "[전체 완료]\n"
     ]
    }
   ],
   "source": [
    "def load_preprocess_data(path: str, sheet_name=0) -> pd.DataFrame:\n",
    "    usecols = ['category', '날짜', '제목', '본문', 'url']\n",
    "    df = pd.read_excel(\n",
    "        path,\n",
    "        sheet_name=sheet_name,\n",
    "        engine=\"openpyxl\",\n",
    "        dtype=str,\n",
    "        usecols=usecols\n",
    "    )\n",
    "\n",
    "    out = preprocess_df(df)\n",
    "    # 중복 제거\n",
    "    out = out.drop_duplicates(subset=['clean_hash'])\n",
    "    # 한국어 비율 0.3 미만, 본문 10자 미만 제거\n",
    "    out = out[(out['ko_ratio'] >= 0.3) & (out['len_chars'] >= 10)].reset_index(drop=True)\n",
    "    # 날짜 파싱/정규화\n",
    "    if '날짜' in out.columns:\n",
    "        out['date'] = pd.to_datetime(out['날짜'], errors='coerce')\n",
    "        out['ym'] = out['date'].dt.to_period('M').astype(str)\n",
    "    return out\n",
    "\n",
    "TARGET_FILES = [\n",
    "    \"1. (SOCIAL)_(개인정보보호법__정보통신망법).xlsx\",\n",
    "    \"2. (SOCIAL)_(자본시장법__특정금융정보법__전자금융거래법__전자증권법__금융소비자보호법).xlsx\",\n",
    "    \"3. (SOCIAL)_(아동복지법).xlsx\",\n",
    "    \"4. (SOCIAL)_(중대재해처벌법).xlsx\",\n",
    "]\n",
    "\n",
    "for path in TARGET_FILES:\n",
    "    part_num = path.split('.')[0]  # \"1\", \"2\", \"3\", \"4\"\n",
    "    print(f\"\\n=== part {part_num} 시작 ===\")\n",
    "\n",
    "    twitter = load_preprocess_data(path, sheet_name=\"twitter\")\n",
    "    twitter.to_excel(f\"preprocess_twitter_part_{part_num}.xlsx\", index=False)\n",
    "\n",
    "    blog = load_preprocess_data(path, sheet_name=\"blog\")\n",
    "    blog.to_excel(f\"preprocess_blog_part_{part_num}.xlsx\", index=False)\n",
    "\n",
    "    insta = load_preprocess_data(path, sheet_name=\"insta\")\n",
    "    insta.to_excel(f\"preprocess_insta_part_{part_num}.xlsx\", index=False)\n",
    "\n",
    "    community = load_preprocess_data(path, sheet_name=\"community\")\n",
    "    community.to_excel(f\"preprocess_community_part_{part_num}.xlsx\", index=False)\n",
    "\n",
    "    print(f\"=== part {part_num} 완료 ===\")\n",
    "\n",
    "print(\"\\n[전체 완료]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차 전처리\n",
    "\n",
    "트위터 - RT,Qt 분리/@,#제거 후 키워드 저장\n",
    "\n",
    "인스타그램 - 본문,해시태그 꼬리 분리/이모지 구분\n",
    "\n",
    "커뮤니티,블로그 - 괄호 정보 제거/\"무단전재,재배포 금지\",\"이메일 전화 제보\" 패턴 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PART 1 시작 ===\n",
      "[INFO] Loading: preprocess_community_part_1.xlsx (platform=community)\n",
      "[OK] Saved: preprocess_community_part_1.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_insta_part_1.xlsx (platform=instagram)\n",
      "[OK] Saved: preprocess_insta_part_1.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_twitter_part_1.xlsx (platform=twitter)\n",
      "[OK] Saved: preprocess_twitter_part_1.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_blog_part_1.xlsx (platform=blog)\n",
      "[OK] Saved: preprocess_blog_part_1.xlsx\n",
      "\n",
      "=== PART 1 완료 ===\n",
      "\n",
      "=== PART 2 시작 ===\n",
      "[INFO] Loading: preprocess_community_part_2.xlsx (platform=community)\n",
      "[OK] Saved: preprocess_community_part_2.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_insta_part_2.xlsx (platform=instagram)\n",
      "[OK] Saved: preprocess_insta_part_2.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_twitter_part_2.xlsx (platform=twitter)\n",
      "[OK] Saved: preprocess_twitter_part_2.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_blog_part_2.xlsx (platform=blog)\n",
      "[OK] Saved: preprocess_blog_part_2.xlsx\n",
      "\n",
      "=== PART 2 완료 ===\n",
      "\n",
      "=== PART 3 시작 ===\n",
      "[INFO] Loading: preprocess_community_part_3.xlsx (platform=community)\n",
      "[OK] Saved: preprocess_community_part_3.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_insta_part_3.xlsx (platform=instagram)\n",
      "[OK] Saved: preprocess_insta_part_3.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_twitter_part_3.xlsx (platform=twitter)\n",
      "[OK] Saved: preprocess_twitter_part_3.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_blog_part_3.xlsx (platform=blog)\n",
      "[OK] Saved: preprocess_blog_part_3.xlsx\n",
      "\n",
      "=== PART 3 완료 ===\n",
      "\n",
      "=== PART 4 시작 ===\n",
      "[INFO] Loading: preprocess_community_part_4.xlsx (platform=community)\n",
      "[OK] Saved: preprocess_community_part_4.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_insta_part_4.xlsx (platform=instagram)\n",
      "[OK] Saved: preprocess_insta_part_4.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_twitter_part_4.xlsx (platform=twitter)\n",
      "[OK] Saved: preprocess_twitter_part_4.xlsx\n",
      "\n",
      "[INFO] Loading: preprocess_blog_part_4.xlsx (platform=blog)\n",
      "[OK] Saved: preprocess_blog_part_4.xlsx\n",
      "\n",
      "=== PART 4 완료 ===\n",
      "\n",
      "[전체 완료]\n"
     ]
    }
   ],
   "source": [
    "import re, hashlib, math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 플랫폼 구분\n",
    "def infer_platform(path: str) -> str:\n",
    "    name = Path(path).name.lower()\n",
    "    if \"twitter\" in name:\n",
    "        return \"twitter\"\n",
    "    if \"insta\" in name:\n",
    "        return \"instagram\"\n",
    "    if \"blog\" in name:\n",
    "        return \"blog\"\n",
    "    if \"community\" in name:\n",
    "        return \"community\"\n",
    "    return \"generic\"\n",
    "\n",
    "# --------- 정규식/룰 ---------\n",
    "RE_RT = re.compile(r'^RT\\s+@\\w+')\n",
    "RE_MENTION = re.compile(r'@(\\w+)')\n",
    "RE_HASHTAG = re.compile(r'#([A-Za-z가-힣0-9_]+)')\n",
    "RE_URL = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "RE_EMAIL = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+')\n",
    "RE_PHONE = re.compile(r'(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\d{2,4}[-.\\s]?){2,3}\\d{3,4}')\n",
    "RE_ZW = re.compile(r'[\\u200d\\uFE0F]')  # zero-width, variation selector-16\n",
    "RE_MULTISPACE = re.compile(r'\\s+')\n",
    "\n",
    "# 노이즈 대괄호 컨텐츠 구분 및 제거\n",
    "RE_NOISE_BRACKETS = re.compile(\n",
    "    r'\\[(?:앵커|기자|사진|영상|카카오톡|메일|전화|무단.*?금지)\\]'\n",
    ")\n",
    "\n",
    "# 한글 비율 연산\n",
    "def korean_ratio(s: str) -> float:\n",
    "    if not s:\n",
    "        return 0.0\n",
    "    total = len(s)\n",
    "    ko = sum(1 for ch in s if '\\uac00' <= ch <= '\\ud7a3')\n",
    "    return ko / total\n",
    "\n",
    "RE_SENT_SPLIT = re.compile(\n",
    "    r'(?:(?<=[\\.!?])\\s+)'\n",
    "    r'|'\n",
    "    r'(?:[다요까죠])\\s*(?:\\n+|\\s*$)'\n",
    ")\n",
    "\n",
    "def sentence_split(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    # 분리 토큰을 기준으로 split\n",
    "    parts = re.split(RE_SENT_SPLIT, s)\n",
    "    return [p.strip() for p in parts if p and p.strip()]\n",
    "\n",
    "def sentence_count(s: str) -> int:\n",
    "    return max(1, len(sentence_split(s)))\n",
    "\n",
    "# 본문-해시태그 꼬리 분리\n",
    "def split_tail_hashtags(text: str):\n",
    "    if not text:\n",
    "        return text, []\n",
    "    lines = [ln.rstrip() for ln in text.splitlines()]\n",
    "    tail_tags = []\n",
    "    # 뒤에서부터 해시태그만 있는 라인을 수집\n",
    "    i = len(lines) - 1\n",
    "    while i >= 0 and lines[i] and all(tok.startswith('#') for tok in lines[i].split()):\n",
    "        tail_tags.extend(RE_HASHTAG.findall(lines[i]))\n",
    "        i -= 1\n",
    "    body = \"\\n\".join(lines[:i+1]).strip()\n",
    "    return (body if body else text), list(dict.fromkeys(tail_tags))  # uniq 유지 순서존중\n",
    "\n",
    "# URL/이메일/전화 토큰화, 노이즈 제거, zero-width 제거, 공백정리\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = RE_NOISE_BRACKETS.sub(\" \", s)\n",
    "    s = RE_URL.sub(\" <URL> \", s)\n",
    "    s = RE_EMAIL.sub(\" <EMAIL> \", s)\n",
    "    s = RE_PHONE.sub(\" <PHONE> \", s)\n",
    "    s = RE_ZW.sub(\"\", s)\n",
    "    # 과도한 공백/개행 축소\n",
    "    s = RE_MULTISPACE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 플랫폼별 메타 추출 및 정규화 정책\n",
    "def platform_process(platform: str, raw_text: str):\n",
    "    text = str(raw_text or \"\")\n",
    "    is_retweet = False\n",
    "    mentions = RE_MENTION.findall(text)\n",
    "    hashtags = RE_HASHTAG.findall(text)\n",
    "\n",
    "    # instagram: 꼬리 해시태그 블록 분리\n",
    "    tail_hashtags = []\n",
    "    if platform == \"instagram\":\n",
    "        base, tail = split_tail_hashtags(text)\n",
    "        # 본문을 base로 교체, 해시태그는 본문+꼬리 합집합\n",
    "        text = base\n",
    "        if tail:\n",
    "            tail_hashtags = tail\n",
    "            hashtags = list(dict.fromkeys(hashtags + tail_hashtags))\n",
    "\n",
    "    # 모델 입력용 텍스트: 멘션 제거, 해시태그는 평문 변환\n",
    "    text_for_model = RE_MENTION.sub(\" \", text)\n",
    "    text_for_model = re.sub(RE_HASHTAG, r\"\\1\", text_for_model)\n",
    "\n",
    "    return {\n",
    "        \"mentions\": mentions,\n",
    "        \"hashtags\": hashtags,\n",
    "        \"text_for_model\": text_for_model\n",
    "    }\n",
    "\n",
    "# 해시(중복 탐지용)\n",
    "def sha1_hex(s: str) -> str:\n",
    "    return hashlib.sha1((s or \"\").encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def process_file(path: str):\n",
    "    platform = infer_platform(path)\n",
    "    print(f\"[INFO] Loading: {path} (platform={platform})\")\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    def safe_str(x):\n",
    "        return x if isinstance(x, str) else \"\"\n",
    "\n",
    "    out_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        title_raw = safe_str(row.get(\"title_norm\", \"\"))\n",
    "        content_raw = safe_str(row.get(\"content_norm\", \"\"))\n",
    "\n",
    "        title_clean_base = clean_text(title_raw)\n",
    "        content_clean_base = clean_text(content_raw)\n",
    "\n",
    "        # 멘션/해시태그/RT는 메타 추출용 합본 사용\n",
    "        combined_for_meta = (title_clean_base + \"\\n\" + content_clean_base).strip()\n",
    "        meta = platform_process(platform, combined_for_meta)\n",
    "\n",
    "        # 멘션 제거, 해시태그 평문화\n",
    "        title_for_model = RE_MENTION.sub(\" \", title_clean_base)\n",
    "        title_for_model = re.sub(RE_HASHTAG, r\"\\1\", title_for_model)\n",
    "        title_final = clean_text(title_for_model)\n",
    "\n",
    "        content_for_model = RE_MENTION.sub(\" \", content_clean_base)\n",
    "        content_for_model = re.sub(RE_HASHTAG, r\"\\1\", content_for_model)\n",
    "        content_final = clean_text(content_for_model)\n",
    "\n",
    "        # 지표/통계는 합본 기준으로 계산(길이/문장수/한글비율 등)\n",
    "        text_for_stats = (title_final + (\"\\n\" if title_final and content_final else \"\") + content_final).strip()\n",
    "        ko_ratio = korean_ratio(text_for_stats)\n",
    "        n_sents = sentence_count(text_for_stats)\n",
    "\n",
    "        out_rows.append({\n",
    "            # 업데이트될 전처리 텍스트\n",
    "            \"title_norm__updated\": title_final,\n",
    "            \"content_norm__updated\": content_final,\n",
    "\n",
    "            # 메타/지표\n",
    "            \"mentions\": meta[\"mentions\"],\n",
    "            \"hashtags\": meta[\"hashtags\"],\n",
    "            \"ko_ratio\": ko_ratio,\n",
    "            \"len_sentences\": n_sents,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(out_rows)\n",
    "\n",
    "    # 원본 주요 컬럼 유지\n",
    "    keep_cols = []\n",
    "    for col in [\"category\", \"title\", \"content\", \"title_norm\", \"content_norm\", \"date\", \"created_at\", \"url\", \"author\", \"likes\", \"retweets\", \"comments\"]:\n",
    "        if col in df.columns:\n",
    "            keep_cols.append(col)\n",
    "\n",
    "    merged = pd.concat([df[keep_cols].reset_index(drop=True), out], axis=1)\n",
    "\n",
    "    # title_norm, content_norm을 전처리 결과로 덮어쓰기\n",
    "    merged[\"title_norm\"] = merged[\"title_norm__updated\"]\n",
    "    merged[\"content_norm\"] = merged[\"content_norm__updated\"]\n",
    "    merged.drop(columns=[\"title_norm__updated\", \"content_norm__updated\"], inplace=True)\n",
    "\n",
    "    # 저장\n",
    "    p = Path(path)\n",
    "    out_xlsx = str(p.with_name(p.stem + \".xlsx\"))\n",
    "\n",
    "    merged.to_excel(out_xlsx, index=False)\n",
    "    print(f\"[OK] Saved: {out_xlsx}\\n\")\n",
    "    return merged\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for part in [1, 2, 3, 4]:\n",
    "    print(f\"\\n=== PART {part} 시작 ===\")\n",
    "    INPUT_FILES = [\n",
    "        f\"preprocess_community_part_{part}.xlsx\",\n",
    "        f\"preprocess_insta_part_{part}.xlsx\",\n",
    "        f\"preprocess_twitter_part_{part}.xlsx\",\n",
    "        f\"preprocess_blog_part_{part}.xlsx\",\n",
    "    ]\n",
    "\n",
    "    for f in INPUT_FILES:\n",
    "        if not Path(f).exists():\n",
    "            print(f\"[SKIP] Missing: {f}\")\n",
    "            continue\n",
    "        try:\n",
    "            all_results[Path(f).name] = process_file(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f}: {e}\")\n",
    "\n",
    "    print(f\"=== PART {part} 완료 ===\")\n",
    "\n",
    "print(\"\\n[전체 완료]\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "law-issue2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
